(function (global, factory) {
  if (typeof define === "function" && define.amd) {
    define(["exports", "@babel/runtime/helpers/classCallCheck", "@babel/runtime/helpers/createClass", "@tensorflow/tfjs", "franc", "langs", "./utils", "./constants"], factory);
  } else if (typeof exports !== "undefined") {
    factory(exports, require("@babel/runtime/helpers/classCallCheck"), require("@babel/runtime/helpers/createClass"), require("@tensorflow/tfjs"), require("franc"), require("langs"), require("./utils"), require("./constants"));
  } else {
    var mod = {
      exports: {}
    };
    factory(mod.exports, global.classCallCheck, global.createClass, global.tfjs, global.franc, global.langs, global.utils, global.constants);
    global.preprocessing = mod.exports;
  }
})(this, function (_exports, _classCallCheck2, _createClass2, tf, _franc, _langs, _utils, _constants) {
  "use strict";

  var _interopRequireWildcard = require("@babel/runtime/helpers/interopRequireWildcard");

  var _interopRequireDefault = require("@babel/runtime/helpers/interopRequireDefault");

  Object.defineProperty(_exports, "__esModule", {
    value: true
  });
  _exports.padSequences = padSequences;
  _exports.detectLang = detectLang;
  _exports.parseUtterance = parseUtterance;
  _exports.preprocessData = preprocessData;
  _exports.Tokenizer = void 0;
  _classCallCheck2 = _interopRequireDefault(_classCallCheck2);
  _createClass2 = _interopRequireDefault(_createClass2);
  tf = _interopRequireWildcard(tf);
  _franc = _interopRequireDefault(_franc);
  _langs = _interopRequireDefault(_langs);

  var Tokenizer =
  /*#__PURE__*/
  function () {
    function Tokenizer() {
      var vocabulary = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : null;
      (0, _classCallCheck2["default"])(this, Tokenizer);
      this.charsToShift = ["'", '.', ',', '?', '!'];
      this.vocabulary = vocabulary;
      this.vocabularyLength = vocabulary ? Object.keys(this.vocabulary).length : null;
      this.minSeqLength = null;
      this.maxSeqLength = null;
    }

    (0, _createClass2["default"])(Tokenizer, [{
      key: "tokenize",
      value: function tokenize(text) {
        var shifted = shiftSpecialChars(text, (0, _utils.clone)(this.charsToShift));
        return shifted.toLowerCase().split(' ');
      }
    }, {
      key: "tokenizeSamples",
      value: function tokenizeSamples(samples) {
        var _this = this;

        return samples.map(function (sample) {
          return _this.tokenize(sample);
        });
      }
    }, {
      key: "fitOnSamples",
      value: function fitOnSamples(samples) {
        var tokenizedSamples = this.tokenizeSamples(samples);
        this.vocabulary = generateVocabulary(tokenizedSamples);
        this.vocabularyLength = Object.keys(this.vocabulary).length;
      }
    }, {
      key: "samplesToSequences",
      value: function samplesToSequences(samples) {
        var tokenizedSamples = [];
        var sequences = [];

        if (typeof samples === 'string') {
          samples = [samples];
        }

        tokenizedSamples = this.tokenizeSamples(samples);
        var _iteratorNormalCompletion = true;
        var _didIteratorError = false;
        var _iteratorError = undefined;

        try {
          for (var _iterator = tokenizedSamples[Symbol.iterator](), _step; !(_iteratorNormalCompletion = (_step = _iterator.next()).done); _iteratorNormalCompletion = true) {
            var tokenizedSample = _step.value;
            var sequence = [];
            var _iteratorNormalCompletion2 = true;
            var _didIteratorError2 = false;
            var _iteratorError2 = undefined;

            try {
              for (var _iterator2 = tokenizedSample[Symbol.iterator](), _step2; !(_iteratorNormalCompletion2 = (_step2 = _iterator2.next()).done); _iteratorNormalCompletion2 = true) {
                var token = _step2.value;

                if (!(token in this.vocabulary)) {
                  sequence.push(this.vocabulary[_constants.UNKNOWN_TOKEN]);
                } else {
                  sequence.push(this.vocabulary[token]);
                }
              }
            } catch (err) {
              _didIteratorError2 = true;
              _iteratorError2 = err;
            } finally {
              try {
                if (!_iteratorNormalCompletion2 && _iterator2["return"] != null) {
                  _iterator2["return"]();
                }
              } finally {
                if (_didIteratorError2) {
                  throw _iteratorError2;
                }
              }
            }

            sequences.push(sequence);
          }
        } catch (err) {
          _didIteratorError = true;
          _iteratorError = err;
        } finally {
          try {
            if (!_iteratorNormalCompletion && _iterator["return"] != null) {
              _iterator["return"]();
            }
          } finally {
            if (_didIteratorError) {
              throw _iteratorError;
            }
          }
        }

        var _getSeqLengths = getSeqLengths(sequences),
            minSeqLength = _getSeqLengths.minSeqLength,
            maxSeqLength = _getSeqLengths.maxSeqLength;

        this.minSeqLength = minSeqLength;
        this.maxSeqLength = maxSeqLength;
        return sequences;
      }
    }]);
    return Tokenizer;
  }();

  _exports.Tokenizer = Tokenizer;

  function shiftSpecialChars(userInput, charsToShift) {
    if (charsToShift.length == 0) {
      return userInput;
    } else {
      var _char = charsToShift.pop();

      var newInput = (0, _utils.replaceAll)(userInput, "".concat(_char), " ".concat(_char));
      return shiftSpecialChars(newInput, charsToShift);
    }
  }

  function generateVocabulary(tokenizedSamples) {
    var vocabulary = {};
    vocabulary[_constants.UNKNOWN_TOKEN] = 0;
    var c = 1; // Reserved for UNKNOWN_TOKEN

    var _iteratorNormalCompletion3 = true;
    var _didIteratorError3 = false;
    var _iteratorError3 = undefined;

    try {
      for (var _iterator3 = tokenizedSamples[Symbol.iterator](), _step3; !(_iteratorNormalCompletion3 = (_step3 = _iterator3.next()).done); _iteratorNormalCompletion3 = true) {
        var tokenizedSample = _step3.value;
        var _iteratorNormalCompletion4 = true;
        var _didIteratorError4 = false;
        var _iteratorError4 = undefined;

        try {
          for (var _iterator4 = tokenizedSample[Symbol.iterator](), _step4; !(_iteratorNormalCompletion4 = (_step4 = _iterator4.next()).done); _iteratorNormalCompletion4 = true) {
            var token = _step4.value;

            if (!(token in vocabulary)) {
              vocabulary[token] = c;
              c++;
            }
          }
        } catch (err) {
          _didIteratorError4 = true;
          _iteratorError4 = err;
        } finally {
          try {
            if (!_iteratorNormalCompletion4 && _iterator4["return"] != null) {
              _iterator4["return"]();
            }
          } finally {
            if (_didIteratorError4) {
              throw _iteratorError4;
            }
          }
        }
      }
    } catch (err) {
      _didIteratorError3 = true;
      _iteratorError3 = err;
    } finally {
      try {
        if (!_iteratorNormalCompletion3 && _iterator3["return"] != null) {
          _iterator3["return"]();
        }
      } finally {
        if (_didIteratorError3) {
          throw _iteratorError3;
        }
      }
    }

    return vocabulary;
  }

  function getSeqLengths(sequences) {
    var seqLengths = [];
    var _iteratorNormalCompletion5 = true;
    var _didIteratorError5 = false;
    var _iteratorError5 = undefined;

    try {
      for (var _iterator5 = sequences[Symbol.iterator](), _step5; !(_iteratorNormalCompletion5 = (_step5 = _iterator5.next()).done); _iteratorNormalCompletion5 = true) {
        var sequence = _step5.value;
        seqLengths.push(sequence.length);
      }
    } catch (err) {
      _didIteratorError5 = true;
      _iteratorError5 = err;
    } finally {
      try {
        if (!_iteratorNormalCompletion5 && _iterator5["return"] != null) {
          _iterator5["return"]();
        }
      } finally {
        if (_didIteratorError5) {
          throw _iteratorError5;
        }
      }
    }

    return {
      minSeqLength: Math.min.apply(null, seqLengths),
      maxSeqLength: Math.max.apply(null, seqLengths)
    };
  }

  function padSequences(sequences, maxSeqLength) {
    var paddedSequences = [];
    var _iteratorNormalCompletion6 = true;
    var _didIteratorError6 = false;
    var _iteratorError6 = undefined;

    try {
      for (var _iterator6 = sequences[Symbol.iterator](), _step6; !(_iteratorNormalCompletion6 = (_step6 = _iterator6.next()).done); _iteratorNormalCompletion6 = true) {
        var sequence = _step6.value;
        var t = tf.tensor1d(sequence).pad([[maxSeqLength - sequence.length, 0]]);
        paddedSequences.push(t);
      }
    } catch (err) {
      _didIteratorError6 = true;
      _iteratorError6 = err;
    } finally {
      try {
        if (!_iteratorNormalCompletion6 && _iterator6["return"] != null) {
          _iterator6["return"]();
        }
      } finally {
        if (_didIteratorError6) {
          throw _iteratorError6;
        }
      }
    }

    return tf.stack(paddedSequences);
  }

  function detectLang(input, languages) {
    var res = (0, _franc["default"])(input, {
      whitelist: languages.map(function (l) {
        return _langs["default"].where('1', l)[3];
      })
    });

    if (res === 'und') {
      return languages[0];
    }

    return _langs["default"].where('3', res)[1];
  }
  /**
   * Given a training example utterance extracts its entities (if exist)
   * and returns a valid utterance for training with its entities.
   * E.g.:
   * Input: 'I would like to go to [Barcelona](Place)'
   * Output:
   * parsedUtterance: 'I would like to go to Barcelona',
   * parsedEntities: [ { raw: '[Barcelona](Place)', value: 'Barcelona', type: 'Place' } ]
   * @param {string} utterance Training example utterance
   * @returns {object} {parsedUtterance, parsedEntities}
   */


  function parseUtterance(utterance) {
    var capturedGroups = utterance.match(_constants.GLOBAL_ENTITIES_REGEX) || [];
    var parsedEntities = capturedGroups.map(function (matched) {
      return _constants.ENTITIES_REGEX.exec(matched);
    }).map(function (parsedEntity) {
      return {
        raw: parsedEntity[0],
        value: parsedEntity[1],
        type: parsedEntity[2]
      };
    });
    var _iteratorNormalCompletion7 = true;
    var _didIteratorError7 = false;
    var _iteratorError7 = undefined;

    try {
      for (var _iterator7 = parsedEntities[Symbol.iterator](), _step7; !(_iteratorNormalCompletion7 = (_step7 = _iterator7.next()).done); _iteratorNormalCompletion7 = true) {
        var entity = _step7.value;
        utterance = utterance.replace(entity.raw, entity.value);
      }
    } catch (err) {
      _didIteratorError7 = true;
      _iteratorError7 = err;
    } finally {
      try {
        if (!_iteratorNormalCompletion7 && _iterator7["return"] != null) {
          _iterator7["return"]();
        }
      } finally {
        if (_didIteratorError7) {
          throw _iteratorError7;
        }
      }
    }

    return {
      parsedUtterance: utterance,
      parsedEntities: parsedEntities
    };
  }

  function preprocessData(devIntents, params) {
    var _getShuffledSamplesAn = getShuffledSamplesAndLabels(devIntents.intents),
        samples = _getShuffledSamplesAn.samples,
        labels = _getShuffledSamplesAn.labels;

    var tokenizer = new Tokenizer();
    tokenizer.fitOnSamples(samples);
    var sequences = tokenizer.samplesToSequences(samples);
    var seqLength = params.MAX_SEQ_LENGTH || tokenizer.maxSeqLength;
    params.MAX_SEQ_LENGTH = seqLength;
    var tensorData = padSequences(sequences, seqLength);
    console.log("Shape of data tensor: [".concat(tensorData.shape, "]"));
    var tensorLabels = tf.oneHot(tf.tensor1d(labels, 'int32'), Object.keys(devIntents.intentsDict).length);
    console.log("Shape of label tensor: [".concat(tensorLabels.shape, "]"));
    var vocabularyLength = tokenizer.vocabularyLength;
    console.log("Found ".concat(vocabularyLength, " unique tokens"));
    return {
      tensorData: tensorData,
      tensorLabels: tensorLabels,
      vocabulary: tokenizer.vocabulary,
      vocabularyLength: tokenizer.vocabularyLength
    };
  }

  function getShuffledSamplesAndLabels(intents) {
    var samples = intents.map(function (intent) {
      return intent.utterance;
    });
    var labels = intents.map(function (intent) {
      return intent.label;
    });
    (0, _utils.shuffle)(samples, labels);
    return {
      samples: samples,
      labels: labels
    };
  }
});